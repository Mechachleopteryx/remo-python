{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Instance Segmentation with Detectron2 and Remo\n",
    "\n",
    "In this tutorial, we do transfer learning on a MaskRCNN model from Detectron2. \n",
    "We use Remo to facilitate exporing, accessing and managing the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxqxkISHjEjg"
   },
   "source": [
    "In particular, we will:\n",
    "\n",
    "* Browse through our images and annotations\n",
    "* Quickly visualize the main properties of the dataset and annotations\n",
    "* Create a train, test, valid split without moving data around, using Remo image tags.\n",
    "* Fine tune a pre-trained MaskRCNN model from Detectron2 and do some inference\n",
    "* Visually compare Mask predictions with the ground truth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2swBcOUSja50"
   },
   "source": [
    "**Along the way, we will see how browsing images, annotations and predictions helps to gather insights on the dataset and on the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QGTs985jhMo"
   },
   "source": [
    "Before proceeding, we need to install the required dependencies. \n",
    "\n",
    "This can be done by executing the next cell. Once complete, **restart your runtime** to ensure that the installed packages can be detected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9_FzH13EjseR",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cloning into 'detectron2_repo'...\nremote: Enumerating objects: 31, done.\u001b[K\nremote: Counting objects: 100% (31/31), done.\u001b[K\nremote: Compressing objects: 100% (19/19), done.\u001b[K\nremote: Total 7887 (delta 8), reused 22 (delta 8), pack-reused 7856\u001b[K\nReceiving objects: 100% (7887/7887), 3.47 MiB | 3.04 MiB/s, done.\nResolving deltas: 100% (5640/5640), done.\n"
    }
   ],
   "source": [
    "#!pip install imantics\n",
    "#!pip install git+https://github.com/facebookresearch/fvcore.git\n",
    "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
    "#!pip install -e detectron2_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24117,
     "status": "ok",
     "timestamp": 1602097027893,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "hwzVRHRHZ7h3",
    "outputId": "63d9571e-a9b4-4a64-8b28-0687cf7b6756"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "GDRIVE_ROOT = \"/gdrive\"\n",
    "drive.mount(GDRIVE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyShnBdbZ8Hx"
   },
   "outputs": [],
   "source": [
    "!pip install \"/gdrive/My Drive/remo-0.5.2.1-py3-none-any.whl\"\n",
    "!python -m remo_app init --colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ0_LPkjj9Xe"
   },
   "source": [
    "Let us then import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1602098681523,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "ZyAvNCJMmvFF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import remo\n",
    "remo.set_viewer('jupyter')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "from imantics import Polygons, Mask\n",
    "\n",
    "import torch, torchvision\n",
    "\n",
    "# Detectron 2 files\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data.datasets import register_coco_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ68o1uzkJAc"
   },
   "source": [
    "### Adding Data to Remo\n",
    "\n",
    "* The Dataset used is a subset of the <a href=\"https://cocodataset.org/#home\">MS COCO Dataset</a>.\n",
    "* The directory structure of the dataset is:\n",
    "```\n",
    "├── instance_segmentation_dataset\n",
    "    ├── images\n",
    "        ├── image_1.jpg\n",
    "        ├── image_2.jpg\n",
    "        ├── ...\n",
    "    ├── annotations\n",
    "        ├── dataset_annotations.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uXtPd40lbx0N",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--2020-10-28 13:30:04--  https://s-3.s3-eu-west-1.amazonaws.com/instance_segmentation_dataset.zip\nResolving s-3.s3-eu-west-1.amazonaws.com (s-3.s3-eu-west-1.amazonaws.com)... 52.218.100.8\nConnecting to s-3.s3-eu-west-1.amazonaws.com (s-3.s3-eu-west-1.amazonaws.com)|52.218.100.8|:443...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 1720189 (1.6M) [application/zip]\nSaving to: ‘instance_segmentation_dataset.zip’\n\ninstance_segmentati 100%[===================>]   1.64M  1.06MB/s    in 1.5s    \n\n2020-10-28 13:30:06 (1.06 MB/s) - ‘instance_segmentation_dataset.zip’ saved [1720189/1720189]\n\nArchive:  instance_segmentation_dataset.zip\n   creating: instance_segmentation_dataset/\n   creating: instance_segmentation_dataset/annotations/\n  inflating: instance_segmentation_dataset/annotations/coco_subset.json  \n   creating: instance_segmentation_dataset/images/\n  inflating: instance_segmentation_dataset/images/000000520301.jpg  \n  inflating: instance_segmentation_dataset/images/000000289343.jpg  \n  inflating: instance_segmentation_dataset/images/000000061471.jpg  \n  inflating: instance_segmentation_dataset/images/000000472375.jpg  \n  inflating: instance_segmentation_dataset/images/000000250758.jpg  \n  inflating: instance_segmentation_dataset/images/000000562121.jpg  \n  inflating: instance_segmentation_dataset/images/000000486046.jpg  \n  inflating: instance_segmentation_dataset/images/000000349302.jpg  \n  inflating: instance_segmentation_dataset/images/000000509735.jpg  \n  inflating: instance_segmentation_dataset/images/000000064359.jpg  \n"
    }
   ],
   "source": [
    "if not os.path.exists('instance_segmentation_dataset.zip'):\n",
    "  !wget https://s-3.s3-eu-west-1.amazonaws.com/instance_segmentation_dataset.zip\n",
    "  !unzip instance_segmentation_dataset.zip\n",
    "else:\n",
    "  print('Files already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1602098700505,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "0Hz7k5N0cW6U"
   },
   "outputs": [],
   "source": [
    "path_to_annotations = 'instance_segmentation_dataset/annotations/'\n",
    "path_to_images = 'instance_segmentation_dataset/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4c2IKhvk4tP"
   },
   "source": [
    "## Train / test split\n",
    "In Remo, we can use tags to organise our images. Among other things, this allows us to generate train / test splits without the need to move image files around.\n",
    "\n",
    "To do this, we just need to pass a dictionary (mapping tags to the relevant images paths) to the function ```remo.generate_image_tags()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1602098700509,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "tvlQdeFIgtGu",
    "outputId": "401be35c-5a68-4754-ae3e-c3b920bfc12b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'instance_segmentation_dataset/annotations/images_tags.csv'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "im_list = [i for i in glob.glob(path_to_images + '/**/*.jpg', recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "train_idx = round(len(im_list) * 0.8)\n",
    "test_idx  = train_idx + round(len(im_list) * 0.2)\n",
    "\n",
    "tags_dict =  {'train' : im_list[0:train_idx], \n",
    "              'test' : im_list[train_idx:test_idx]}\n",
    "\n",
    "train_test_split_file_path = os.path.join(path_to_annotations, 'images_tags.csv') \n",
    "remo.generate_image_tags(tags_dictionary  = tags_dict, \n",
    "                         output_file_path = train_test_split_file_path, \n",
    "                         append_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyCLB5AdlA9q"
   },
   "source": [
    "### Create a dataset\n",
    "\n",
    "To create a dataset we can use ```remo.create_dataset()```, specifying the path to data and annotations.\n",
    "\n",
    "For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b6yii4wdZ3C6",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Acquiring data - completed                                                                           \nProcessing annotation files: 1 of 2 filesProcessing data - completed                                                                          \nData upload completed with some errors:\n000000289343.jpg: No valid url\n000000061471.jpg: No valid url\n000000472375.jpg: No valid url\n000000520301.jpg: No valid url\n000000064359.jpg: No valid url\n000000486046.jpg: No valid url\n000000349302.jpg: No valid url\n000000509735.jpg: No valid url\n000000250758.jpg: No valid url\n000000562121.jpg: No valid url\n"
    }
   ],
   "source": [
    "instance_segmentation_dataset = remo.create_dataset(name = 'coco_subset', local_files = [path_to_annotations, path_to_images], annotation_task='Instance Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhtI7LI5lTda"
   },
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view and explore images and labels, we can use Remo directly from the notebook. We just need to call ```dataset.view()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vM85hXjDZ3C_",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/datasets/2\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_b9857316-3944-4f0f-9b69-0bf889a6b21e\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/datasets/2?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_b9857316-3944-4f0f-9b69-0bf889a6b21e\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "instance_segmentation_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiQ4iX-MmBym"
   },
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Using Remo, we can quickly visualize some key Dataset properties that can help us with our modelling, without needing to write extra boilerplate code.\n",
    "\n",
    "This can be done either from code, or using the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mPuePhS7mDho"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'AnnotationSet ID': 3,\n  'AnnotationSet name': 'Instance segmentation',\n  'n_images': 10,\n  'n_classes': 10,\n  'n_objects': 35,\n  'top_3_classes': [{'name': 'Zebra', 'count': 13},\n   {'name': 'Giraffe', 'count': 9},\n   {'name': 'Dog', 'count': 4}],\n  'creation_date': None,\n  'last_modified_date': '2020-10-28T13:31:29.901116Z'}]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "instance_segmentation_dataset.get_annotation_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PcTk9j6GmFb5",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/annotation-detail/3/insights\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_8614784a-5db8-4473-bbbf-da964412176b\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/annotation-detail/3/insights?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_8614784a-5db8-4473-bbbf-da964412176b\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "instance_segmentation_dataset.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvRkcddYlXlF"
   },
   "source": [
    "**Exporting the dataset**\n",
    "\n",
    "To export annotations according to the train, test split in a format accepted by the model, we use the ```dataset.export_annotations_to_file()``` method, and filter by the desired tag.\n",
    "\n",
    "For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1602098960894,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "1nco9a1liIjG"
   },
   "outputs": [],
   "source": [
    "path_to_train = path_to_annotations + \"instance_segmentation_train.json\"\n",
    "path_to_test = path_to_annotations + \"instance_segmentation_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1602098961788,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "X522eyE0Z3DE"
   },
   "outputs": [],
   "source": [
    "instance_segmentation_dataset.export_annotations_to_file(path_to_train, annotation_format='coco', filter_by_tags=['train'], export_tags=False, append_path=False)\n",
    "instance_segmentation_dataset.export_annotations_to_file(path_to_test, annotation_format='coco', filter_by_tags=['test'], export_tags=False, append_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcsvm-6ZZ3DJ"
   },
   "source": [
    "# Detectron2\n",
    "\n",
    "Here we will start working with the ```Detectron2``` framework written in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvIw6apnUuf"
   },
   "source": [
    "## Feeding Data into Detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6zVCtb1mwNt"
   },
   "source": [
    "To use Detectron2, you are required to register your dataset.\n",
    "\n",
    "The ```register_coco_instances``` method takes in the following parameters:\n",
    "\n",
    "* **path_to_annotations:** Path to annotation files. Format: COCO JSON.\n",
    "* **path_to_images:** Path to the folder containing the images.\n",
    "\n",
    "This then allows to store the metadata for future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1602098994157,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "Lnkg1PByUjGQ"
   },
   "outputs": [],
   "source": [
    "register_coco_instances('instance_segmentation_train', {}, path_to_train, path_to_images)\n",
    "register_coco_instances('instance_segmentation_test', {}, path_to_test, path_to_images)\n",
    "\n",
    "train_metadata = MetadataCatalog.get('instance_segmentation_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OTMNIVZnTUQ"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "For the sake of the tutorial, our ```Mask RCNN``` architecture will have a ```ResNet-50 Backbone```, pre-trained on on COCO train2017. This can be loaded directly from Detectron2.\n",
    "\n",
    "To train the model, we specify the following details:\n",
    "\n",
    "- **model_yaml_path:** Configuration file for the Mask RCNN model.\n",
    "- **model_weights_path**: Symbolic link to the desired Mask RCNN architecture.\n",
    "\n",
    "The parameters can be tweaked by overriding the correspodning variable in the ```cfg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "colab_instance_segmentation_tutorial.ipynb  instance_segmentation_dataset.zip\n\u001b[0m\u001b[01;34minstance_segmentation_dataset\u001b[0m/\n"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7unkuuiqLdqd"
   },
   "outputs": [],
   "source": [
    "model_yaml_path = './detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'\n",
    "model_weights_path = 'detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl'\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_yaml_path)\n",
    "cfg.DATASETS.TRAIN = ('instance_segmentation_train',)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_weights_path # initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.02\n",
    "cfg.SOLVER.MAX_ITER = 150    # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRqEn61uovp2"
   },
   "source": [
    "**Instantiating the Trainer**\n",
    "\n",
    "We instatiate the trainer with the required configuration, and finally kick-off the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "prcuWITMou0w",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[10/28 13:44:14 d2.engine.defaults]: \u001b[0mModel:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (predictor): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/28 13:44:14 d2.data.datasets.coco]: \u001b[0m\nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n\n\u001b[32m[10/28 13:44:14 d2.data.datasets.coco]: \u001b[0mLoaded 8 images in COCO format from instance_segmentation_dataset/annotations/instance_segmentation_train.json\n\u001b[32m[10/28 13:44:14 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 8 images left.\n\u001b[32m[10/28 13:44:14 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|   person   | 1            |  bicycle   | 2            | motorcycle | 1            |\n|   bench    | 1            |    dog     | 3            |   zebra    | 10           |\n|  giraffe   | 7            |   bottle   | 1            |    cup     | 2            |\n|   toilet   | 1            |            |              |            |              |\n|   total    | 29           |            |              |            |              |\u001b[0m\n\u001b[32m[10/28 13:44:14 d2.data.common]: \u001b[0mSerializing 8 elements to byte tensors and concatenating them all ...\n\u001b[32m[10/28 13:44:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n\u001b[32m[10/28 13:44:14 d2.data.dataset_mapper]: \u001b[0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n\u001b[32m[10/28 13:44:14 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\nSkip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (10, 256, 1, 1) in the model! You might want to double check if this is expected.\nSkip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n\u001b[32m[10/28 13:44:15 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n\u001b[32m[10/28 13:44:39 d2.utils.events]: \u001b[0m eta: 0:02:32  iter: 19  total_loss: 3.581  loss_cls: 1.945  loss_box_reg: 0.945  loss_mask: 0.677  loss_rpn_cls: 0.007  loss_rpn_loc: 0.013  time: 1.1777  data_time: 0.0142  lr: 0.000400  max_mem: 2720M\n\u001b[32m[10/28 13:45:03 d2.utils.events]: \u001b[0m eta: 0:02:08  iter: 39  total_loss: 2.214  loss_cls: 0.735  loss_box_reg: 0.920  loss_mask: 0.494  loss_rpn_cls: 0.006  loss_rpn_loc: 0.013  time: 1.1740  data_time: 0.0067  lr: 0.000799  max_mem: 2720M\n\u001b[32m[10/28 13:45:27 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 59  total_loss: 1.297  loss_cls: 0.273  loss_box_reg: 0.756  loss_mask: 0.267  loss_rpn_cls: 0.001  loss_rpn_loc: 0.012  time: 1.1930  data_time: 0.0063  lr: 0.001199  max_mem: 2720M\n\u001b[32m[10/28 13:45:51 d2.utils.events]: \u001b[0m eta: 0:01:24  iter: 79  total_loss: 0.929  loss_cls: 0.135  loss_box_reg: 0.595  loss_mask: 0.205  loss_rpn_cls: 0.000  loss_rpn_loc: 0.013  time: 1.1885  data_time: 0.0065  lr: 0.001598  max_mem: 2720M\n\u001b[32m[10/28 13:46:14 d2.utils.events]: \u001b[0m eta: 0:01:00  iter: 99  total_loss: 0.697  loss_cls: 0.121  loss_box_reg: 0.403  loss_mask: 0.183  loss_rpn_cls: 0.000  loss_rpn_loc: 0.011  time: 1.1841  data_time: 0.0062  lr: 0.001998  max_mem: 2720M\n\u001b[32m[10/28 13:46:38 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 119  total_loss: 0.589  loss_cls: 0.081  loss_box_reg: 0.360  loss_mask: 0.155  loss_rpn_cls: 0.000  loss_rpn_loc: 0.008  time: 1.1870  data_time: 0.0064  lr: 0.002398  max_mem: 2720M\n\u001b[32m[10/28 13:47:03 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 139  total_loss: 0.546  loss_cls: 0.080  loss_box_reg: 0.351  loss_mask: 0.138  loss_rpn_cls: 0.000  loss_rpn_loc: 0.009  time: 1.1904  data_time: 0.0069  lr: 0.002797  max_mem: 2720M\n\u001b[32m[10/28 13:47:16 d2.utils.events]: \u001b[0m eta: 0:00:01  iter: 149  total_loss: 0.698  loss_cls: 0.108  loss_box_reg: 0.377  loss_mask: 0.188  loss_rpn_cls: 0.001  loss_rpn_loc: 0.011  time: 1.1904  data_time: 0.0066  lr: 0.002997  max_mem: 2720M\n\u001b[32m[10/28 13:47:16 d2.engine.hooks]: \u001b[0mOverall training speed: 147 iterations in 0:02:56 (1.1985 s / it)\n\u001b[32m[10/28 13:47:16 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:57 (0:00:01 on hooks)\n"
    }
   ],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0aT_wYKpIGu"
   },
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can easily browse our predictions and compare them with the ground-truth.\n",
    "\n",
    "We will do this by uploading the model predictions to a new ```AnnotationSet```, which we call `model_predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1602099003560,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "UJ6K3CCHZ3DV"
   },
   "outputs": [],
   "source": [
    "mapping = {k: v for k, v in enumerate(train_metadata.thing_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ya5nEuMELeq8",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/28 13:50:36 d2.data.datasets.coco]: \u001b[0m\nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n\n\u001b[32m[10/28 13:50:36 d2.data.datasets.coco]: \u001b[0mLoaded 2 images in COCO format from instance_segmentation_dataset/annotations/instance_segmentation_test.json\n"
    }
   ],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"instance_segmentation_test\", )\n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_dataset_dicts = DatasetCatalog.get(\"instance_segmentation_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1602098310327,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "OuHpdPcNomu4"
   },
   "outputs": [],
   "source": [
    "for d in test_dataset_dicts:    \n",
    "    im = np.array(Image.open(d[\"file_name\"]))\n",
    "    outputs = predictor(im)\n",
    "    pred_classes = outputs['instances'].get('pred_classes').cpu().numpy()\n",
    "    masks = outputs['instances'].get(\"pred_masks\").cpu().permute(1, 2, 0).numpy()\n",
    "    image_name = d['file_name']\n",
    "    annotations = []\n",
    "    \n",
    "    if masks.shape[2] != 0:\n",
    "        for i in range(masks.shape[2]):\n",
    "            polygons = Mask(masks[:, :, i]).polygons()\n",
    "            annotation = remo.Annotation()\n",
    "            annotation.img_filename = image_name\n",
    "            annotation.classes = mapping[pred_classes[i]]\n",
    "            annotation.segment = polygons.segmentation[0]\n",
    "            annotations.append(annotation)\n",
    "    else:\n",
    "        polygons = Mask(masks[:, :, 0]).polygons()\n",
    "        annotation = remo.Annotation()\n",
    "        annotation.img_filename = image_name\n",
    "        annotation.classes = mapping[pred_classes[0]]\n",
    "        annotation.segment = polygons.segmentation[0]\n",
    "        annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation_dataset = remo.get_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3292,
     "status": "ok",
     "timestamp": 1602097960944,
     "user": {
      "displayName": "Sree Harsha Nelaturu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64",
      "userId": "01158735529041422505"
     },
     "user_tz": -330
    },
    "id": "VrzA6mcJdRQ0",
    "outputId": "311ed274-4bf7-49c7-acfe-b9e6ca51bba4",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Progress 100% - 1/1 - elapsed 0:00:00.001000 - speed: 1000.00 img / s, ETA: 0:00:00\nAcquiring data - completed                                                                           \nProcessing data - completed                                                                          \nData upload completed\n"
    }
   ],
   "source": [
    "model_predictions = instance_segmentation_dataset.create_annotation_set(annotation_task = 'Instance Segmentation', name = 'model_predictions')\n",
    "\n",
    "instance_segmentation_dataset.add_annotations(annotations, annotation_set_id=model_predictions.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DfazUplreHoN",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/datasets/3\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_2303e2ed-8622-4f4a-8050-cedb9471146e\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/datasets/3?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_2303e2ed-8622-4f4a-8050-cedb9471146e\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "instance_segmentation_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqYP2rlEpjJw"
   },
   "source": [
    "By visualizing the predicted masks against the ground truth, we can go past summary performance metrics, and visually inspect model biases and iterate to improve it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "colab_instance_segmentation_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python_defaultSpec_1603891672978"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}