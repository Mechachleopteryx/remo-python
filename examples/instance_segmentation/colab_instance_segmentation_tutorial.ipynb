{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_instance_segmentation_tutorial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vM54r6jlKTII"},"source":["# Instance Segmentation Pipeline using Remo\n","\n","In this tutorial, we will use Remo to accelerate and improve the process of building a transfer learning pipeline for an Instance Segmentation task.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jxqxkISHjEjg"},"source":["In particular, we will:\n","\n","* Use Remo to browse through our images and annotations\n","* Use Remo to understand the properties of the dataset and annotations by  visualizing statistics.\n","* Create a custom train, test, valid split in-place using Remo image tags.\n","* Fine tune a pre-trained MaskRCNN model from Detectron2 and do some inference\n","* Visually compare Mask predictions with the ground truth\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2swBcOUSja50"},"source":["**Along the way, we will see how the Dataset visualization provided Remo helps to gather insights to improve the dataset and the model.**"]},{"cell_type":"markdown","metadata":{"id":"3QGTs985jhMo"},"source":["Before proceeding with the tutorial, we would need to install the required dependencies. This can be done by executing the next cell.\n","\n","Once complete, **restart your runtime**, to ensure that the installed packages can be detected.\n","\n"]},{"cell_type":"code","metadata":{"id":"9_FzH13EjseR","tags":[]},"source":["!pip install imantics\n","!pip install git+https://github.com/facebookresearch/fvcore.git\n","!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n","!pip install -e detectron2_repo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwzVRHRHZ7h3","executionInfo":{"status":"ok","timestamp":1602097027893,"user_tz":-330,"elapsed":24117,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}},"outputId":"63d9571e-a9b4-4a64-8b28-0687cf7b6756","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","GDRIVE_ROOT = \"/gdrive\"\n","drive.mount(GDRIVE_ROOT)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZyShnBdbZ8Hx"},"source":["!pip install \"/gdrive/My Drive/remo-0.5.2.1-py3-none-any.whl\"\n","!python -m remo_app init --colab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJ0_LPkjj9Xe"},"source":["Let us then import the required packages."]},{"cell_type":"code","metadata":{"id":"ZyAvNCJMmvFF","executionInfo":{"status":"ok","timestamp":1602098681523,"user_tz":-330,"elapsed":1280,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["import remo\n","remo.set_viewer('jupyter')\n","\n","import numpy as np\n","import os\n","from PIL import Image\n","import glob\n","import random\n","random.seed(4)\n","\n","from imantics import Polygons, Mask\n","\n","import torch, torchvision\n","\n","# Detectron 2 files\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","from detectron2.engine import DefaultTrainer\n","from detectron2.data.datasets import register_coco_instances"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZ68o1uzkJAc"},"source":["### Adding Data to Remo\n","\n","* The Dataset used is a subset of the MS-COCO Dataset.\n","* The directory structure of the dataset is as follows\n","```\n","├── instance_segmentation_dataset\n","    ├── images\n","        ├── image_1.jpg\n","        ├── image_2.jpg\n","        ├── ...\n","    ├── annotations\n","        ├── dataset_annotations.json\n","```\n"]},{"cell_type":"code","metadata":{"id":"uXtPd40lbx0N"},"source":["if not os.path.exists('object_detection_dataset.zip'):\n","  !unzip instance_segmentation_dataset.zip\n","else:\n","  print('Files already downloaded')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Hz7k5N0cW6U","executionInfo":{"status":"ok","timestamp":1602098700505,"user_tz":-330,"elapsed":1252,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["path_to_annotations = 'instance_segmentation_dataset/annotations/'\n","path_to_images = 'instance_segmentation_dataset/images/'"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R4c2IKhvk4tP"},"source":["## Train / test split\n","In Remo, we can use tags to organise our images. Among other things, this allows us to generate train / test splits without the need to move image files around.\n","\n","To do this, we just need to pass a dictionary (mapping tags to the relevant images paths) to the function ```remo.generate_image_tags()```."]},{"cell_type":"code","metadata":{"id":"tvlQdeFIgtGu","executionInfo":{"status":"ok","timestamp":1602098700509,"user_tz":-330,"elapsed":691,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}},"outputId":"401be35c-5a68-4754-ae3e-c3b920bfc12b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["im_list = [i for i in glob.glob(path_to_images + '/**/*.jpg', recursive=True)]\n","im_list = random.sample(im_list, len(im_list))\n","\n","train_idx = round(len(im_list) * 0.4)\n","valid_idx = train_idx + round(len(im_list) * 0.3)\n","test_idx  = valid_idx + round(len(im_list) * 0.3)\n","\n","tags_dict =  {'train' : im_list[0:train_idx], \n","              'valid' : im_list[train_idx:valid_idx], \n","              'test' : im_list[valid_idx:test_idx]}\n","\n","train_test_split_file_path = os.path.join(path_to_annotations, 'images_tags.csv') \n","remo.generate_image_tags(tags_dictionary  = tags_dict, \n","                         output_file_path = train_test_split_file_path, \n","                         append_path = False)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'instance_segmentation_dataset/annotations/images_tags.csv'"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"jyCLB5AdlA9q"},"source":["### Create a dataset\n","\n","To create a dataset we can use ```remo.create_dataset()```, specifying the path to data and annotations.\n","\n","The class encoding (if required) is passed via a dictionary.\n","\n","For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>.\n"]},{"cell_type":"code","metadata":{"tags":[],"id":"b6yii4wdZ3C6"},"source":["instance_segmentation_dataset = remo.create_dataset(name = 'instance_segmentation_dataset', local_files = [path_to_annotations, path_to_images], annotation_task='Instance Segmentation')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yhtI7LI5lTda"},"source":["**Visualizing the dataset**\n","\n","To view and explore images and labels, we can use Remo directly from the notebook. We just need to call ```dataset.view()```."]},{"cell_type":"code","metadata":{"tags":[],"id":"vM85hXjDZ3C_"},"source":["instance_segmentation_dataset.view()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiQ4iX-MmBym"},"source":["**Dataset Statistics**\n","\n","Using Remo, we can quickly visualize some key Dataset properties that can help us with our modelling, without needing to write extra boilerplate code.\n","\n","This can be done either from code, or using the visual interface."]},{"cell_type":"code","metadata":{"id":"mPuePhS7mDho"},"source":["instance_segmentation_dataset.get_annotation_statistics()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcTk9j6GmFb5"},"source":["instance_segmentation_dataset.view_annotation_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvRkcddYlXlF"},"source":["**Exporting the dataset**\n","\n","To export annotations according to the train, test split in a format accepted by the model, we use the ```dataset.export_annotations_to_file()``` method, and filter by the desired tag.\n","\n","For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>."]},{"cell_type":"code","metadata":{"id":"1nco9a1liIjG","executionInfo":{"status":"ok","timestamp":1602098960894,"user_tz":-330,"elapsed":1344,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["path_to_train = path_to_annotations + \"dog_train.json\"\n","path_to_test = path_to_annotations + \"dog_test.json\""],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"X522eyE0Z3DE","executionInfo":{"status":"ok","timestamp":1602098961788,"user_tz":-330,"elapsed":2110,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["instance_segmentation_dataset.export_annotations_to_file(path_to_train, annotation_format='coco', filter_by_tags=['train'], export_tags=False, append_path=False)\n","instance_segmentation_dataset.export_annotations_to_file(path_to_test, annotation_format='coco', filter_by_tags=['test'], export_tags=False, append_path=False)"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcsvm-6ZZ3DJ"},"source":["# Detectron2\n","\n","Here we will start working with the ```Detectron2``` framework written in PyTorch."]},{"cell_type":"markdown","metadata":{"id":"ucvIw6apnUuf"},"source":["## Feeding Data into Detectron2"]},{"cell_type":"markdown","metadata":{"id":"d6zVCtb1mwNt"},"source":["First, you are required to register your dataset, Detectron2 assumes your data is in MS-COCO format.\n","\n","The ```register_coco_instances``` method takes in the following parameters:\n","\n","* **path_to_annotations:** Path to annotation files. Format: COCO JSON.\n","* **path_to_images:** Path to the folder containing the images.\n","\n","After this, we store the metadata for future operations."]},{"cell_type":"code","metadata":{"id":"Lnkg1PByUjGQ","executionInfo":{"status":"ok","timestamp":1602098994157,"user_tz":-330,"elapsed":922,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["register_coco_instances('dog_trainn', {}, path_to_train, path_to_images)\n","register_coco_instances('dog_testt', {}, path_to_test, path_to_images)\n","\n","train_metadata = MetadataCatalog.get('dog_train')"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d81cQY-ukyvX"},"source":["To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them."]},{"cell_type":"code","metadata":{"id":"UJ6K3CCHZ3DV","executionInfo":{"status":"ok","timestamp":1602099003560,"user_tz":-330,"elapsed":962,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["mapping = {k: v for k, v in enumerate(train_metadata.thing_classes)}"],"execution_count":61,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4OTMNIVZnTUQ"},"source":["## Training the Model\n","\n","In this tutorial, we use a ```Mask RCNN``` architecture with a ```ResNet-50 Backbone```, pre-trained on on COCO train2017. This is loaded directly from Detectron2.\n","\n","To train the model, we specify the following details:\n","\n","- **model_yaml_path:** Configuration file for the Mask RCNN model.\n","- **model_weights_path**: Symbolic link to the desired Mask RCNN architecture.\n","\n","The parameters can be tweaked by overriding the correspodning variable in the ```cfg```."]},{"cell_type":"code","metadata":{"id":"7unkuuiqLdqd"},"source":["model_yaml_path = './detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'\n","model_weights_path = 'detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl'\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_yaml_path)\n","cfg.DATASETS.TRAIN = ('dog_train',)\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_weights_path # initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.02\n","cfg.SOLVER.MAX_ITER = 150    # 300 iterations seems good enough, but you can certainly train longer\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 3 classes (data, fig, hazelnut)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRqEn61uovp2"},"source":["**Instantiating the Trainer**\n","\n","The trainer is instantiated with the required configuration, and the training process is started."]},{"cell_type":"code","metadata":{"id":"prcuWITMou0w"},"source":["os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg)\n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0aT_wYKpIGu"},"source":["## Visualizing Predictions\n","\n","Using Remo, we can easily iterate through the images to compare the model predictions against the original labels.\n","\n","To do this, we just need to upload the model predictions to a new ```AnnotationSet```, which we call `model_predictions`"]},{"cell_type":"code","metadata":{"id":"Ya5nEuMELeq8"},"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n","cfg.DATASETS.TEST = (\"dog_test\", )\n","predictor = DefaultPredictor(cfg)\n","test_dataset_dicts = DatasetCatalog.get(\"dog_test\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OuHpdPcNomu4","executionInfo":{"status":"ok","timestamp":1602098310327,"user_tz":-330,"elapsed":1066,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}}},"source":["for d in test_dataset_dicts:    \n","    im = np.array(Image.open(d[\"file_name\"]))\n","    outputs = predictor(im)\n","    pred_classes = outputs['instances'].get('pred_classes').cpu().numpy()\n","    masks = outputs['instances'].get(\"pred_masks\").cpu().permute(1, 2, 0).numpy()\n","    image_name = d['file_name']\n","    annotations = []\n","    \n","    if masks.shape[2] != 0:\n","        for i in range(masks.shape[2]):\n","            polygons = Mask(masks[:, :, i]).polygons()\n","            annotation = remo.Annotation()\n","            annotation.img_filename = image_name\n","            annotation.classes = mapping[pred_classes[i]]\n","            annotation.segment = polygons.segmentation[0]\n","            annotations.append(annotation)\n","    else:\n","        polygons = Mask(masks[:, :, 0]).polygons()\n","        annotation = remo.Annotation()\n","        annotation.img_filename = image_name\n","        annotation.classes = mapping[pred_classes[0]]\n","        annotation.segment = polygons.segmentation[0]\n","        annotations.append(annotation)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrzA6mcJdRQ0","executionInfo":{"status":"ok","timestamp":1602097960944,"user_tz":-330,"elapsed":3292,"user":{"displayName":"Sree Harsha Nelaturu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghyid0sxSGIX-vJDOaqVZv_5TgJ_muLwOFbqtQLZQ=s64","userId":"01158735529041422505"}},"outputId":"311ed274-4bf7-49c7-acfe-b9e6ca51bba4","colab":{"base_uri":"https://localhost:8080/"}},"source":["model_predictions = instance_segmentation_dataset.create_annotation_set(annotation_task = 'Instance Segmentation', name = 'model_predictions')\n","\n","instance_segmentation_dataset.add_annotations(annotations, annotation_set_id=model_predictions.id)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Progress 100% - 1/1 - elapsed 0:00:00.001000 - speed: 1000.00 img / s, ETA: 0:00:00\n","Acquiring data - completed                                                                           \n","Processing data - completed                                                                          \n","Data upload completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DfazUplreHoN"},"source":["instance_segmentation_dataset.view()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqYP2rlEpjJw"},"source":["By visualizing the predicted masks against the ground truth, we can go past summary performance metrics, and visually inspect model biases and iterate to improve it."]}]}